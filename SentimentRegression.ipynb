{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMWjSlk-MAD3"
      },
      "source": [
        "###IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWKcn_waL9yz"
      },
      "source": [
        "import ntpath\n",
        "from os import walk, path\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "# PREPROCESSING related imports\n",
        "\n",
        "from random import shuffle\n",
        "import re    \n",
        "import string  \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples                          \n",
        "from nltk.stem import PorterStemmer        \n",
        "from nltk.tokenize import TweetTokenizer   \n",
        "from nltk.corpus import stopwords          \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prutWifBMD7-"
      },
      "source": [
        "###GETTING DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-_l2p6uLQkQ"
      },
      "source": [
        "dir = '.'\n",
        "data_p = \"/content/\"\n",
        "train_p = data_p + 'Dataset/train/pos/' \n",
        "train_n = data_p + 'Dataset/train/neg/'\n",
        "\n",
        "def download_data():\n",
        "  !gdown --id 1pwbVIT5yyUbQZTKp6Fy3gir9eOsUj3nB --output \"{dir}/data.zip\"\n",
        "\n",
        "def unzip_data():\n",
        "  f = dir+\"/data.zip\"\n",
        "  !mkdir \"$data_p\"\n",
        "  !unzip \"$f\" -d \"$data_p\"\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqgKEwwTMI0K"
      },
      "source": [
        "###HELPER METHODS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Her8LXhmMQWe"
      },
      "source": [
        "def getF(directory, x=\"both\"):\n",
        "    # x could be \"files\" or \"folders\" or \"both\"\n",
        "\n",
        "    folders = []\n",
        "    for f in walk(directory):\n",
        "        folders.extend(f)\n",
        "\n",
        "    if x == \"folders\":\n",
        "\n",
        "      return folders[1]\n",
        "      N\n",
        "      \n",
        "        \n",
        "    if x == \"files\":\n",
        "        return folders[2]\n",
        "    else:\n",
        "      return folders[1], folders[2]\n",
        "\n",
        "\n",
        "\n",
        "# read all text files\n",
        "\n",
        "def read_Alltxt(read_path):\n",
        "  files = getF(read_path,'files')\n",
        "  txtFiles = []\n",
        "  for f in files:\n",
        "    with open(read_path+f, 'r', encoding=\"utf-8\") as of:\n",
        "      data = ''\n",
        "      for line in of:\n",
        "        data += line\n",
        "      txtFiles.append(data)\n",
        "  return txtFiles\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def BOW(f_as_ftokens, labels):\n",
        "    bow = {}\n",
        "\n",
        "    for tokens, label in list(zip(f_as_ftokens, labels)):\n",
        "        for token in tokens:\n",
        "            bow[(token, label)] = bow.get((token, label), 0) + 1\n",
        "        \n",
        "    return bow\n",
        "\n",
        "\n",
        "\n",
        "# feature vectors\n",
        "def extract_features(f_as_ftokens, bow):\n",
        "    # feature array\n",
        "    features = np.zeros((1,3))\n",
        "    # bias term added in the 0th index\n",
        "    features[0,0] = 1\n",
        "    \n",
        "    # iterate processed_tweet\n",
        "    for word in f_as_ftokens:\n",
        "        # get the positive frequency of the word\n",
        "        features[0,1] = bow.get((word, 1), 0)\n",
        "        # get the negative frequency of the word\n",
        "        features[0,2] = bow.get((word, 0), 0)\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jKFdHtvNNpn"
      },
      "source": [
        "###PREPROCESSOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4yNvFu_NQsh"
      },
      "source": [
        "class Preprocessor():\n",
        "    def __init__(self):\n",
        "      self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                                      reduce_len=True)\n",
        "      nltk.download('stopwords')\n",
        "      self.stopwords_en = stopwords.words('english') \n",
        "      self.punctuation_en = string.punctuation\n",
        "      self.stemmer = PorterStemmer() \n",
        "\n",
        "    def __clean_n_tokenize_text__(self, txt):\n",
        "\n",
        "        # remove hyperlinks\n",
        "        txt = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', txt)\n",
        "        # remove hashtags\n",
        "        txt = re.sub(r'#', '', txt)\n",
        "        #remove email address\n",
        "        txt = re.sub('\\S+@\\S+', '', txt)\n",
        "        # remove numbers\n",
        "        txt = re.sub(r'\\d+', '', txt)\n",
        "        txt =  self.tokenizer.tokenize(txt)\n",
        "        return txt\n",
        "\n",
        "\n",
        "    def __filter_stopwords__(self, tokens):\n",
        "        # remove stopwords\n",
        "        filtered_tokens = []\n",
        "\n",
        "        for word in tokens:\n",
        "            if (word not in self.stopwords_en and  # remove stopwords\n",
        "                word not in self.punctuation_en):  # remove punctuation\n",
        "                filtered_tokens.append(word)\n",
        "        return filtered_tokens\n",
        "\n",
        "    def __stem_tokens__(self, tokens):\n",
        "        # store the stemmed word\n",
        "        stemmed_tokens = [] \n",
        "\n",
        "        for word in tokens:\n",
        "            stem_word = self.stemmer.stem(word)  \n",
        "            stemmed_tokens.append(stem_word)\n",
        "        return stemmed_tokens\n",
        "\n",
        "\n",
        "\n",
        "    def preprocess(self, txts):\n",
        "        final_tokens = []\n",
        "        for _, txt in tqdm(enumerate(txts)):        \n",
        "            txt = self.__clean_n_tokenize_text__(txt)                       \n",
        "            txt = self.__filter_stopwords__(txt)\n",
        "            txt = self.__stem_tokens__(txt)\n",
        "            final_tokens.extend([txt])\n",
        "        return final_tokens"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZup28_GObUV"
      },
      "source": [
        "###MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYZMRtmENs1z"
      },
      "source": [
        "class Model():\n",
        "  @staticmethod\n",
        "  def sigmoid(z): \n",
        "      return 1 / (1+ np.exp(-z))\n",
        "\n",
        "\n",
        "  def gradientDescent(self, x, y, theta, alpha, num_iters, c):\n",
        "      # total samples\n",
        "      m = x.shape[0]\n",
        "      \n",
        "      for i in range(0, num_iters):\n",
        "          z = np.dot(x, theta)\n",
        "          h = Model.sigmoid(z)\n",
        "          cost = (-1/m) * ((np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1-h))) + (c * np.sum(theta)))\n",
        "          \n",
        "          theta = theta - (alpha / m) * np.dot((x.T), (h - y))\n",
        "    \n",
        "      cost = float(cost)\n",
        "      return cost, theta\n",
        "\n",
        "\n",
        "  def predict(self, x, theta):\n",
        "      return Model.sigmoid(np.dot(x, theta))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywcGN2qFOj7v"
      },
      "source": [
        "###TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SEX5X0sPoU5"
      },
      "source": [
        "#####Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-FyOujrPgdi"
      },
      "source": [
        "download_data()\n",
        "unzip_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKS8YFGWNCZI"
      },
      "source": [
        "#####READ TEXT FILES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enpMDgvZNHPX"
      },
      "source": [
        "data_p = read_Alltxt(train_p)\n",
        "\n",
        "data_n = read_Alltxt(train_n)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVDQzzVqP1l-"
      },
      "source": [
        "#####Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br46MFRtMSrA",
        "outputId": "05f46d67-d65e-406b-c64d-9319950bf2bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "processor = Preprocessor()\n",
        "\n",
        "# process the positive and negative tweets\n",
        "pfs_as_ftokens = processor.preprocess(data_p)\n",
        "nfs_as_ftokens = processor.preprocess(data_n)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19it [00:00, 186.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12500it [00:49, 251.72it/s]\n",
            "12500it [00:48, 259.43it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPyW4bS7MSwO",
        "outputId": "4308e590-9a79-459d-ad78-babf50362575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pfs_as_ftokens[0:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['upon',\n",
              "  'first',\n",
              "  'view',\n",
              "  'found',\n",
              "  'tale',\n",
              "  'least',\n",
              "  'less',\n",
              "  'annoy',\n",
              "  'cannon',\n",
              "  'movi',\n",
              "  'tale',\n",
              "  'mani',\n",
              "  'think',\n",
              "  'one',\n",
              "  'best',\n",
              "  'song',\n",
              "  'pretti',\n",
              "  'bad',\n",
              "  'especi',\n",
              "  'love',\n",
              "  'song',\n",
              "  'two',\n",
              "  'thing',\n",
              "  'stand',\n",
              "  'make',\n",
              "  'movi',\n",
              "  'even',\n",
              "  'sing',\n",
              "  'worthwhil',\n",
              "  'one',\n",
              "  'art',\n",
              "  'direct',\n",
              "  'like',\n",
              "  'cannon',\n",
              "  'movi',\n",
              "  'tale',\n",
              "  'beauti',\n",
              "  'decor',\n",
              "  'period',\n",
              "  'piec',\n",
              "  'everi',\n",
              "  'piec',\n",
              "  'cloth',\n",
              "  'jewel',\n",
              "  'major',\n",
              "  'part',\n",
              "  \"movie'\",\n",
              "  'plot',\n",
              "  'look',\n",
              "  'fresh',\n",
              "  'new',\n",
              "  'contrast',\n",
              "  'plain',\n",
              "  'cloth',\n",
              "  'peasant',\n",
              "  'even',\n",
              "  'love',\n",
              "  'song',\n",
              "  'find',\n",
              "  'studi',\n",
              "  'dress',\n",
              "  'hair',\n",
              "  'princess',\n",
              "  'wonder',\n",
              "  'done',\n",
              "  'thing',\n",
              "  'comic',\n",
              "  'time',\n",
              "  'lot',\n",
              "  'movi',\n",
              "  'cheesi',\n",
              "  \"emperor'\",\n",
              "  'vaniti',\n",
              "  'make',\n",
              "  'fun',\n",
              "  'end',\n",
              "  'suspici',\n",
              "  'guard',\n",
              "  'guard',\n",
              "  'chase',\n",
              "  'nichola',\n",
              "  'stupid',\n",
              "  'princ',\n",
              "  'quit',\n",
              "  'funni',\n",
              "  'seem',\n",
              "  'ridicul',\n",
              "  'quit',\n",
              "  'purpos',\n",
              "  'sequenc',\n",
              "  'song',\n",
              "  'weave-o',\n",
              "  'make',\n",
              "  'song',\n",
              "  'good'],\n",
              " ['came',\n",
              "  'back',\n",
              "  'montreal',\n",
              "  'premier',\n",
              "  'zero',\n",
              "  'day',\n",
              "  '...',\n",
              "  \"i'm\",\n",
              "  'surpris',\n",
              "  'hell',\n",
              "  'find',\n",
              "  'neg',\n",
              "  'comment',\n",
              "  'movi',\n",
              "  'basic',\n",
              "  'blame',\n",
              "  'coccio',\n",
              "  'easi',\n",
              "  'overplay',\n",
              "  'social',\n",
              "  'messag',\n",
              "  '...',\n",
              "  'well',\n",
              "  \"mr-i'm-a-review\",\n",
              "  'easi',\n",
              "  'overplay',\n",
              "  'critic',\n",
              "  'movi',\n",
              "  'social',\n",
              "  'charg',\n",
              "  'br',\n",
              "  'br',\n",
              "  'want',\n",
              "  'expos',\n",
              "  'life',\n",
              "  'come',\n",
              "  'small',\n",
              "  'town',\n",
              "  'similar',\n",
              "  'school',\n",
              "  'guy',\n",
              "  'go',\n",
              "  'reject',\n",
              "  'ignor',\n",
              "  'menu',\n",
              "  'thing',\n",
              "  '...',\n",
              "  'understand',\n",
              "  'young',\n",
              "  'kid',\n",
              "  'driven',\n",
              "  'horror',\n",
              "  'high',\n",
              "  'school',\n",
              "  'becam',\n",
              "  'battl',\n",
              "  'field',\n",
              "  'conform',\n",
              "  'real',\n",
              "  'ugli',\n",
              "  'sight',\n",
              "  'need',\n",
              "  'fight',\n",
              "  'way',\n",
              "  'like',\n",
              "  'other',\n",
              "  'hard',\n",
              "  'explain',\n",
              "  'bit',\n",
              "  'lot',\n",
              "  'peopl',\n",
              "  \"dosen't\",\n",
              "  'realiz',\n",
              "  'high',\n",
              "  'school',\n",
              "  'becom',\n",
              "  'cemeteri',\n",
              "  'human',\n",
              "  'intellig',\n",
              "  'meanwhil',\n",
              "  'parent',\n",
              "  'close',\n",
              "  'eye',\n",
              "  'smile',\n",
              "  'life',\n",
              "  'comfort',\n",
              "  'suburb',\n",
              "  'perfect',\n",
              "  'br',\n",
              "  'br',\n",
              "  'real',\n",
              "  'motiv',\n",
              "  'movi',\n",
              "  'drive',\n",
              "  'death-lik',\n",
              "  'calm',\n",
              "  'suburb',\n",
              "  'everybodi',\n",
              "  'close',\n",
              "  'eye',\n",
              "  'tri',\n",
              "  'creat',\n",
              "  'atmospher',\n",
              "  'perfect',\n",
              "  'town',\n",
              "  'cal',\n",
              "  'express',\n",
              "  'well',\n",
              "  'wake',\n",
              "  'call',\n",
              "  'drama',\n",
              "  'everywher',\n",
              "  'take',\n",
              "  'everi',\n",
              "  'shape',\n",
              "  'case',\n",
              "  'littl',\n",
              "  'drama',\n",
              "  'like',\n",
              "  'andr',\n",
              "  'call',\n",
              "  'faggot',\n",
              "  'wear',\n",
              "  'j',\n",
              "  'c',\n",
              "  'penni',\n",
              "  'shirt',\n",
              "  'shape',\n",
              "  'wors',\n",
              "  'nightmar',\n",
              "  'whole',\n",
              "  'town',\n",
              "  'andr',\n",
              "  'cal',\n",
              "  'took',\n",
              "  'extrem',\n",
              "  'way',\n",
              "  'express',\n",
              "  'pain',\n",
              "  'malais',\n",
              "  'unconform',\n",
              "  'era',\n",
              "  'need',\n",
              "  'ever',\n",
              "  'like',\n",
              "  'other',\n",
              "  'accept',\n",
              "  'br',\n",
              "  'br',\n",
              "  'like',\n",
              "  'particularli',\n",
              "  'last',\n",
              "  'scene',\n",
              "  'guy',\n",
              "  'burn',\n",
              "  'cross',\n",
              "  'andr',\n",
              "  'cal',\n",
              "  'like',\n",
              "  'pain',\n",
              "  'commun',\n",
              "  'cal',\n",
              "  'andr',\n",
              "  'commun',\n",
              "  'blind',\n",
              "  'rage',\n",
              "  'commun',\n",
              "  'refus',\n",
              "  'think',\n",
              "  'caus',\n",
              "  'act',\n",
              "  'br',\n",
              "  'br',\n",
              "  'might',\n",
              "  'seem',\n",
              "  'aggress',\n",
              "  'movi',\n",
              "  'coccio',\n",
              "  'medit',\n",
              "  'whine',\n",
              "  'enunci',\n",
              "  'andr',\n",
              "  'cal',\n",
              "  'live',\n",
              "  'realiti',\n",
              "  '...',\n",
              "  'scari',\n",
              "  'one',\n",
              "  'might',\n",
              "  'get',\n",
              "  'kid',\n",
              "  'br',\n",
              "  'br',\n",
              "  'disturb',\n",
              "  'movi',\n",
              "  '...',\n",
              "  'home',\n",
              "  'make',\n",
              "  'strong',\n",
              "  'feel',\n",
              "  'made',\n",
              "  'ben',\n",
              "  'coccio',\n",
              "  'disturb',\n",
              "  'movi'],\n",
              " ['know',\n",
              "  'like',\n",
              "  'film',\n",
              "  'part',\n",
              "  \"other'\",\n",
              "  'mention',\n",
              "  'bit',\n",
              "  'long',\n",
              "  'tooth',\n",
              "  'also',\n",
              "  'found',\n",
              "  'rage',\n",
              "  'hormon',\n",
              "  'male',\n",
              "  'crew',\n",
              "  'bit',\n",
              "  'annoy',\n",
              "  'wonder',\n",
              "  'start',\n",
              "  'pant',\n",
              "  'howl',\n",
              "  'moon',\n",
              "  'well',\n",
              "  'also',\n",
              "  'say',\n",
              "  'overal',\n",
              "  'movi',\n",
              "  'leav',\n",
              "  'cold',\n",
              "  'steril',\n",
              "  'atmospher',\n",
              "  'permeat',\n",
              "  'film',\n",
              "  'plu',\n",
              "  'side',\n",
              "  'effect',\n",
              "  'great',\n",
              "  'besid',\n",
              "  'cartoon',\n",
              "  'monster',\n",
              "  'effect',\n",
              "  'prop',\n",
              "  'costum',\n",
              "  'cours',\n",
              "  'robbi',\n",
              "  'robot',\n",
              "  'film',\n",
              "  'think',\n",
              "  'would',\n",
              "  'popular',\n",
              "  'second',\n",
              "  'half',\n",
              "  'movi',\n",
              "  'pick',\n",
              "  'steam',\n",
              "  'start',\n",
              "  'investig',\n",
              "  'forgotten',\n",
              "  'gadget',\n",
              "  'krell',\n",
              "  'mani',\n",
              "  'time',\n",
              "  \"i'v\",\n",
              "  'seen',\n",
              "  'movi',\n",
              "  'krell',\n",
              "  'still',\n",
              "  'leav',\n",
              "  'scratch',\n",
              "  'head',\n",
              "  'exactli',\n",
              "  'base',\n",
              "  'produc',\n",
              "  'time',\n",
              "  'besid',\n",
              "  'island',\n",
              "  'earth',\n",
              "  'forbidden',\n",
              "  'planet',\n",
              "  'mile',\n",
              "  'averag',\n",
              "  'sci-fi',\n",
              "  'movi',\n",
              "  'time',\n",
              "  'film',\n",
              "  'color',\n",
              "  'also',\n",
              "  'add',\n",
              "  'enjoy',\n",
              "  'certainli',\n",
              "  'classic',\n",
              "  'right',\n",
              "  'flaw',\n",
              "  'deservedli'],\n",
              " ['first',\n",
              "  'like',\n",
              "  'movi',\n",
              "  'caus',\n",
              "  'nazi',\n",
              "  'swastika',\n",
              "  'drama.but',\n",
              "  'buy',\n",
              "  'see',\n",
              "  'bad',\n",
              "  'heard',\n",
              "  'mani',\n",
              "  'complaint',\n",
              "  'number',\n",
              "  'short',\n",
              "  'ils',\n",
              "  'werner',\n",
              "  'sing',\n",
              "  'understand',\n",
              "  'radio',\n",
              "  'show',\n",
              "  'super',\n",
              "  'propaganda',\n",
              "  'radio',\n",
              "  'program',\n",
              "  'ils',\n",
              "  'johann',\n",
              "  'zara',\n",
              "  'plu',\n",
              "  'rudi',\n",
              "  'shruki',\n",
              "  'band',\n",
              "  'like',\n",
              "  'kurt',\n",
              "  'widman',\n",
              "  'orchestra',\n",
              "  'fud',\n",
              "  'cantic',\n",
              "  'ex',\n",
              "  'cetera',\n",
              "  'never',\n",
              "  'appear',\n",
              "  'radio',\n",
              "  'show',\n",
              "  'caus',\n",
              "  'singer',\n",
              "  'band',\n",
              "  'pop',\n",
              "  'jazz',\n",
              "  'swing',\n",
              "  'categori',\n",
              "  'club',\n",
              "  'foot',\n",
              "  'regul',\n",
              "  'tour',\n",
              "  'occupi',\n",
              "  'area',\n",
              "  'soldier',\n",
              "  'short',\n",
              "  'wave',\n",
              "  'radio',\n",
              "  'soldier',\n",
              "  'also',\n",
              "  'night',\n",
              "  'club',\n",
              "  'hotel',\n",
              "  'berlin',\n",
              "  'hamburg',\n",
              "  'record',\n",
              "  'sale',\n",
              "  'ils',\n",
              "  'allow',\n",
              "  'sing',\n",
              "  'pictur',\n",
              "  'would',\n",
              "  'made',\n",
              "  'medium',\n",
              "  'budget',\n",
              "  'music',\n",
              "  'make',\n",
              "  'music',\n",
              "  'would',\n",
              "  'demonstr',\n",
              "  'whistling.but',\n",
              "  'excel',\n",
              "  'exampl',\n",
              "  'propaganda.ing',\n",
              "  'aunt',\n",
              "  'eichhorn',\n",
              "  'play',\n",
              "  'ida',\n",
              "  'wust',\n",
              "  'goe',\n",
              "  'olymp',\n",
              "  'aunt',\n",
              "  'forget',\n",
              "  'ticket',\n",
              "  'ing',\n",
              "  'wait',\n",
              "  'till',\n",
              "  'aunt',\n",
              "  'come',\n",
              "  'back',\n",
              "  'ticket',\n",
              "  'meet',\n",
              "  'carl',\n",
              "  'radditz',\n",
              "  'play',\n",
              "  'herbert',\n",
              "  'extra',\n",
              "  'ticket',\n",
              "  'goe',\n",
              "  'love',\n",
              "  'first',\n",
              "  'sight',\n",
              "  'plan',\n",
              "  'marri',\n",
              "  'spanish',\n",
              "  'war',\n",
              "  'get',\n",
              "  'way',\n",
              "  'go',\n",
              "  'assign',\n",
              "  'right',\n",
              "  'side.carl',\n",
              "  'raddatz',\n",
              "  'mani',\n",
              "  'peopl',\n",
              "  'complain',\n",
              "  'realli',\n",
              "  'handsom',\n",
              "  'plain',\n",
              "  'opfergang',\n",
              "  'put',\n",
              "  'mustach',\n",
              "  'plu',\n",
              "  'suntan',\n",
              "  'made',\n",
              "  'plain',\n",
              "  'looking.y',\n",
              "  'see',\n",
              "  'nazi',\n",
              "  'soldier',\n",
              "  'act',\n",
              "  'normal',\n",
              "  'like',\n",
              "  'scene',\n",
              "  'ex',\n",
              "  'butcher',\n",
              "  'troop',\n",
              "  'franc',\n",
              "  'steal',\n",
              "  'pig',\n",
              "  'farm',\n",
              "  'make',\n",
              "  'lunch',\n",
              "  'leader',\n",
              "  'suggest',\n",
              "  'save',\n",
              "  'pig',\n",
              "  'reflect',\n",
              "  'adolph',\n",
              "  'anim',\n",
              "  'right',\n",
              "  'extrem',\n",
              "  'charact',\n",
              "  'butcher',\n",
              "  'soldier',\n",
              "  'subtl',\n",
              "  'put',\n",
              "  'meat',\n",
              "  'eating.l',\n",
              "  'world',\n",
              "  'war',\n",
              "  'herbert',\n",
              "  'fli',\n",
              "  'german',\n",
              "  'airplan',\n",
              "  'shoot',\n",
              "  'one',\n",
              "  'pilot',\n",
              "  'herbert',\n",
              "  'take',\n",
              "  'shoot',\n",
              "  'plane',\n",
              "  'crash',\n",
              "  'unfortun',\n",
              "  'us',\n",
              "  'survive.anoth',\n",
              "  'seen',\n",
              "  'nazi',\n",
              "  'soldier',\n",
              "  'go',\n",
              "  'bomb',\n",
              "  'cathol',\n",
              "  'church',\n",
              "  'put',\n",
              "  'cathol',\n",
              "  \"hubert'\",\n",
              "  'best',\n",
              "  'friend',\n",
              "  'helmet',\n",
              "  'play',\n",
              "  'joachim',\n",
              "  'brenneck',\n",
              "  'start',\n",
              "  'play',\n",
              "  'organ',\n",
              "  'beethtoven',\n",
              "  'bomb',\n",
              "  'come',\n",
              "  'us',\n",
              "  'church',\n",
              "  'bomb',\n",
              "  'soldier',\n",
              "  'continu',\n",
              "  'stay',\n",
              "  'play',\n",
              "  'organ',\n",
              "  \"he'\",\n",
              "  'told',\n",
              "  'leav',\n",
              "  'end',\n",
              "  'injur',\n",
              "  'propaganda',\n",
              "  'messag',\n",
              "  'cathol',\n",
              "  'church',\n",
              "  'organ',\n",
              "  'caus',\n",
              "  'becom',\n",
              "  'addict',\n",
              "  'it.it',\n",
              "  'injur',\n",
              "  'see',\n",
              "  'time',\n",
              "  'ing',\n",
              "  'either',\n",
              "  'mother',\n",
              "  'grandma',\n",
              "  'play',\n",
              "  'hedwig',\n",
              "  'bleibteu',\n",
              "  'german',\n",
              "  'grandam',\n",
              "  'actress',\n",
              "  'play',\n",
              "  'maria',\n",
              "  \"holst'\",\n",
              "  'aunt',\n",
              "  'weiner',\n",
              "  'blut.wel',\n",
              "  'later',\n",
              "  'come',\n",
              "  'short',\n",
              "  'view',\n",
              "  'radio',\n",
              "  'show',\n",
              "  'intend',\n",
              "  'music',\n",
              "  'revu',\n",
              "  'kora',\n",
              "  'terri',\n",
              "  'releas',\n",
              "  'year',\n",
              "  'well',\n",
              "  'rosen',\n",
              "  'tirol',\n",
              "  'music',\n",
              "  'well',\n",
              "  'side',\n",
              "  'war',\n",
              "  'suppos',\n",
              "  'back',\n",
              "  'drop',\n",
              "  'mainli',\n",
              "  'war',\n",
              "  'romant',\n",
              "  'movie.it',\n",
              "  'easi',\n",
              "  'take',\n",
              "  'pot',\n",
              "  'shot',\n",
              "  'soldier',\n",
              "  'movi',\n",
              "  'real',\n",
              "  'life',\n",
              "  'mani',\n",
              "  'soldier',\n",
              "  'forc',\n",
              "  'fight',\n",
              "  'nazi',\n",
              "  'caus',\n",
              "  'caus',\n",
              "  'job',\n",
              "  'monthli',\n",
              "  'pay',\n",
              "  'would',\n",
              "  'receiv',\n",
              "  'war',\n",
              "  'mani',\n",
              "  'surviv',\n",
              "  'would',\n",
              "  'regret',\n",
              "  'good',\n",
              "  'swastika',\n",
              "  'classic',\n",
              "  'problem',\n",
              "  'today',\n",
              "  'neo',\n",
              "  'nazi',\n",
              "  'nazi',\n",
              "  'skin',\n",
              "  'head',\n",
              "  'watch',\n",
              "  'movi',\n",
              "  'reflect',\n",
              "  'hitler',\n",
              "  'worship',\n",
              "  'disturb',\n",
              "  'websit',\n",
              "  'exploit',\n",
              "  'film',\n",
              "  'classic',\n",
              "  'rais',\n",
              "  'money',\n",
              "  'insan',\n",
              "  'care',\n",
              "  'time',\n",
              "  'direct',\n",
              "  'hate',\n",
              "  'classic',\n",
              "  'scenario',\n",
              "  'look',\n",
              "  'like',\n",
              "  'glorifi',\n",
              "  'nazi',\n",
              "  'websit',\n",
              "  'skip',\n",
              "  'go',\n",
              "  'ihf',\n",
              "  'german',\n",
              "  'wartim',\n",
              "  'film',\n",
              "  'dot',\n",
              "  'com',\n",
              "  'amazon',\n",
              "  'dot',\n",
              "  'dee',\n",
              "  'german',\n",
              "  'video',\n",
              "  'dot',\n",
              "  'net',\n",
              "  'legitim',\n",
              "  'mada',\n",
              "  'mistak',\n",
              "  \"herbert'\",\n",
              "  'friend',\n",
              "  'got',\n",
              "  'kill',\n",
              "  'church',\n",
              "  'malt',\n",
              "  \"yager'\",\n",
              "  \"character'\",\n",
              "  'friend',\n",
              "  'schartzscop'],\n",
              " ['seen',\n",
              "  'origin',\n",
              "  'incred',\n",
              "  'journey',\n",
              "  'sinc',\n",
              "  'child',\n",
              "  \"can't\",\n",
              "  'realli',\n",
              "  'compar',\n",
              "  'two',\n",
              "  'version',\n",
              "  'version',\n",
              "  'tell',\n",
              "  'stori',\n",
              "  'three',\n",
              "  'anim',\n",
              "  'two',\n",
              "  'dog',\n",
              "  'cat',\n",
              "  'whose',\n",
              "  'owner',\n",
              "  'leav',\n",
              "  'friend',\n",
              "  'countrysid',\n",
              "  'father',\n",
              "  'famili',\n",
              "  'take',\n",
              "  'new',\n",
              "  'job',\n",
              "  'san',\n",
              "  'francisco',\n",
              "  'pet',\n",
              "  'believ',\n",
              "  'abandon',\n",
              "  'escap',\n",
              "  'set',\n",
              "  'long',\n",
              "  'homeward',\n",
              "  'journey',\n",
              "  'wilder',\n",
              "  'br',\n",
              "  'br',\n",
              "  'stori',\n",
              "  'might',\n",
              "  'easili',\n",
              "  'film',\n",
              "  'cartoon',\n",
              "  'version',\n",
              "  'fact',\n",
              "  'live-act',\n",
              "  'film',\n",
              "  'made',\n",
              "  'use',\n",
              "  'real',\n",
              "  'anim',\n",
              "  'one',\n",
              "  'major',\n",
              "  'differ',\n",
              "  'later',\n",
              "  'version',\n",
              "  'anim',\n",
              "  'speak',\n",
              "  'human',\n",
              "  'voic',\n",
              "  'give',\n",
              "  'distinct',\n",
              "  'person',\n",
              "  'someth',\n",
              "  'done',\n",
              "  'origin',\n",
              "  'film',\n",
              "  'similar',\n",
              "  'devic',\n",
              "  'talk',\n",
              "  'anim',\n",
              "  'use',\n",
              "  'recent',\n",
              "  \"children'\",\n",
              "  'film',\n",
              "  'race',\n",
              "  'stripe',\n",
              "  'critic',\n",
              "  'rather',\n",
              "  'sniffi',\n",
              "  'use',\n",
              "  'devic',\n",
              "  'view',\n",
              "  'give',\n",
              "  'anim',\n",
              "  'distinct',\n",
              "  'person',\n",
              "  'help',\n",
              "  'strengthen',\n",
              "  'film',\n",
              "  'rather',\n",
              "  'weaken',\n",
              "  'anim',\n",
              "  'voic',\n",
              "  'big-nam',\n",
              "  'star',\n",
              "  'amech',\n",
              "  'michael',\n",
              "  'j',\n",
              "  'fox',\n",
              "  'salli',\n",
              "  'field',\n",
              "  'br',\n",
              "  'br',\n",
              "  'dog',\n",
              "  'male',\n",
              "  'relationship',\n",
              "  'parallel',\n",
              "  'mani',\n",
              "  'human',\n",
              "  'buddy-buddi',\n",
              "  'movi',\n",
              "  'shadow',\n",
              "  'golden',\n",
              "  'retriev',\n",
              "  'wise',\n",
              "  'experienc',\n",
              "  'older',\n",
              "  'dog',\n",
              "  'chanc',\n",
              "  'younger',\n",
              "  'one',\n",
              "  'brash',\n",
              "  'cocki',\n",
              "  'impuls',\n",
              "  'british',\n",
              "  'eye',\n",
              "  'chanc',\n",
              "  'look',\n",
              "  'like',\n",
              "  'boxer',\n",
              "  'actual',\n",
              "  'american',\n",
              "  'bulldog',\n",
              "  'appar',\n",
              "  'differ',\n",
              "  'breed',\n",
              "  'british',\n",
              "  'cousin',\n",
              "  'sassi',\n",
              "  'cat',\n",
              "  'femal',\n",
              "  'rather',\n",
              "  'prim',\n",
              "  'proper',\n",
              "  'person',\n",
              "  'proud',\n",
              "  'statu',\n",
              "  'cat',\n",
              "  'eye',\n",
              "  'make',\n",
              "  'vastli',\n",
              "  'superior',\n",
              "  'mere',\n",
              "  'dog',\n",
              "  'cat',\n",
              "  'rule',\n",
              "  'dog',\n",
              "  'drool',\n",
              "  'br',\n",
              "  'br',\n",
              "  'adult',\n",
              "  'viewpoint',\n",
              "  'film',\n",
              "  'number',\n",
              "  'fault',\n",
              "  'sentiment',\n",
              "  'incid',\n",
              "  'one',\n",
              "  'anim',\n",
              "  'manag',\n",
              "  'catapult',\n",
              "  'mountain',\n",
              "  'lion',\n",
              "  'river',\n",
              "  'quit',\n",
              "  'incred',\n",
              "  'human',\n",
              "  'charact',\n",
              "  'complet',\n",
              "  'forgett',\n",
              "  'howev',\n",
              "  'film',\n",
              "  'mainli',\n",
              "  'aim',\n",
              "  'children',\n",
              "  'suspect',\n",
              "  'enjoy',\n",
              "  'immens',\n",
              "  'certainli',\n",
              "  'animal-lov',\n",
              "  'child',\n",
              "  'comment',\n",
              "  'profession',\n",
              "  'critic',\n",
              "  'jame',\n",
              "  'berardinelli',\n",
              "  'complain',\n",
              "  'anim',\n",
              "  'voic',\n",
              "  'lessen',\n",
              "  \"film'\",\n",
              "  'grandeur',\n",
              "  'serv',\n",
              "  'strengthen',\n",
              "  'view',\n",
              "  'profession',\n",
              "  'critic',\n",
              "  'alway',\n",
              "  'best',\n",
              "  'guid',\n",
              "  \"children'\",\n",
              "  'movi',\n",
              "  'doubt',\n",
              "  'mani',\n",
              "  'playground',\n",
              "  'convers',\n",
              "  'homeward',\n",
              "  'bound',\n",
              "  'concentr',\n",
              "  'suppos',\n",
              "  'grandeur',\n",
              "  'br',\n",
              "  'br',\n",
              "  'one',\n",
              "  'thing',\n",
              "  'adult',\n",
              "  'appreci',\n",
              "  'photographi',\n",
              "  \"california'\",\n",
              "  'sierra',\n",
              "  'nevada',\n",
              "  'mountain',\n",
              "  'may',\n",
              "  'also',\n",
              "  'appreci',\n",
              "  \"film'\",\n",
              "  'blend',\n",
              "  'humour',\n",
              "  'excit',\n",
              "  'runaway',\n",
              "  'pet',\n",
              "  'encount',\n",
              "  'peril',\n",
              "  'bear',\n",
              "  'mountain',\n",
              "  'lion',\n",
              "  'porcupin',\n",
              "  'wilder',\n",
              "  'enjoy',\n",
              "  'famili',\n",
              "  'film']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z2ixIuSNa3x"
      },
      "source": [
        "# Labels\n",
        "\n",
        "labels = [1 for i in range(len(pfs_as_ftokens))]\n",
        "labels.extend([0 for i in range(len(nfs_as_ftokens))])\n",
        "pos_neg_fs_as_ftoken = pfs_as_ftokens + nfs_as_ftokens"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJcML1qSMS4N"
      },
      "source": [
        "bow = BOW(pos_neg_fs_as_ftoken, labels)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwnQ9zs3NslV"
      },
      "source": [
        "df = pd.DataFrame(list(zip(pos_neg_fs_as_ftoken, labels)), columns=[\"f_as_tokens\", \"labels\"])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lxTRXy6N1Kr",
        "outputId": "a6895c36-f0b7-449b-c0a4-0418487df3ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_as_tokens</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[upon, first, view, found, tale, least, less, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[came, back, montreal, premier, zero, day, ......</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[know, like, film, part, other', mention, bit,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[first, like, movi, caus, nazi, swastika, dram...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[seen, origin, incred, journey, sinc, child, c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         f_as_tokens  labels\n",
              "0  [upon, first, view, found, tale, least, less, ...       1\n",
              "1  [came, back, montreal, premier, zero, day, ......       1\n",
              "2  [know, like, film, part, other', mention, bit,...       1\n",
              "3  [first, like, movi, caus, nazi, swastika, dram...       1\n",
              "4  [seen, origin, incred, journey, sinc, child, c...       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR7SKox8N4nt"
      },
      "source": [
        "train_Xt, test_Xt, train_Y, test_Y = train_test_split(df[\"f_as_tokens\"], df[\"labels\"], test_size = 0.2, shuffle=True)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-R4Wqt-N1Tk"
      },
      "source": [
        "train_X = np.zeros((len(train_Xt), 3))\n",
        "\n",
        "for index, f_as_token in enumerate(train_Xt):\n",
        "    train_X[index, :] = extract_features(f_as_token, bow)\n",
        "\n",
        "# test X feature dimension\n",
        "test_X = np.zeros((len(test_Xt), 3))\n",
        "\n",
        "for index, f_as_token in enumerate(test_Xt):\n",
        "    test_X[index, :] = extract_features(f_as_token, bow)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AACqsEkPdYq"
      },
      "source": [
        "#####Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXauxEr0Ns8z",
        "outputId": "39552de0-a1a9-46d2-f131-838442da8980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "model = Model()\n",
        "# 0.1 as L2 \n",
        "cost, theta = model.gradientDescent(train_X, np.array(train_Y).reshape(-1,1), np.zeros((3, 1)), 1e-7, 1000, 0.1)\n",
        "print(f\"Total Cost {cost:.8f}.\")\n",
        "print(f\"Weight Vector {[round(v, 7) for v in np.squeeze(theta)]}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Cost 1.02032667.\n",
            "Weight Vector [-1.8e-06, 0.000505, -0.000633]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuUQm1plOmPo"
      },
      "source": [
        "###TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p17n0typNsu5",
        "outputId": "a32c46a6-ad2f-4f07-e6ce-3e9595a3fece",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "predicted_probs = model.predict(test_X, theta)\n",
        "\n",
        "predicted_labels = np.where(predicted_probs > 0.5, 1, 0)\n",
        "\n",
        "# accuracy\n",
        "print(f\"accuracy is {len(predicted_labels[predicted_labels == np.array(test_Y).reshape(-1,1)]) / len(test_Y)*100:.2f}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 60.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUuKxdiROqoO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}